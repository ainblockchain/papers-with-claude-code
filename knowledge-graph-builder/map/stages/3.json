{
  "map": {
    "type": "map",
    "version": "1.10",
    "orientation": "orthogonal",
    "renderorder": "right-down",
    "width": 20,
    "height": 15,
    "tilewidth": 40,
    "tileheight": 40,
    "infinite": false,
    "layers": [
      {
        "type": "tilelayer",
        "id": 1,
        "name": "floor",
        "data": [
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3
        ],
        "width": 20,
        "height": 15,
        "x": 0,
        "y": 0,
        "visible": true,
        "opacity": 1
      },
      {
        "type": "tilelayer",
        "id": 2,
        "name": "collision",
        "data": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
        ],
        "width": 20,
        "height": 15,
        "x": 0,
        "y": 0,
        "visible": false,
        "opacity": 1
      },
      {
        "type": "objectgroup",
        "id": 3,
        "name": "objects",
        "draworder": "topdown",
        "x": 0,
        "y": 0,
        "visible": true,
        "opacity": 1,
        "objects": [
          {
            "id": 1,
            "name": "spawn",
            "type": "spawn",
            "x": 40,
            "y": 280,
            "width": 0,
            "height": 0,
            "rotation": 0,
            "visible": true,
            "point": true
          },
          {
            "id": 2,
            "name": "door",
            "type": "door",
            "x": 720,
            "y": 280,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true
          },
          {
            "id": 3,
            "name": "encoder_stack",
            "type": "npc",
            "x": 120,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "encoder_stack"
              },
              {
                "name": "title",
                "type": "string",
                "value": "The Encoder: Understanding the Input"
              }
            ]
          },
          {
            "id": 4,
            "name": "position_wise_ffn",
            "type": "npc",
            "x": 240,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "position_wise_ffn"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Position-wise Feed-Forward Networks"
              }
            ]
          },
          {
            "id": 5,
            "name": "masked_attention",
            "type": "npc",
            "x": 360,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "masked_attention"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Masked Self-Attention: No Peeking Ahead"
              }
            ]
          },
          {
            "id": 6,
            "name": "decoder_stack",
            "type": "npc",
            "x": 120,
            "y": 400,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "decoder_stack"
              },
              {
                "name": "title",
                "type": "string",
                "value": "The Decoder: Generating the Output"
              }
            ]
          },
          {
            "id": 7,
            "name": "transformer_architecture",
            "type": "npc",
            "x": 240,
            "y": 400,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "transformer_architecture"
              },
              {
                "name": "title",
                "type": "string",
                "value": "The Complete Transformer"
              }
            ]
          }
        ]
      }
    ],
    "tilesets": [
      {
        "firstgid": 1,
        "name": "course-room-tiles",
        "tilewidth": 40,
        "tileheight": 40,
        "tilecount": 3,
        "columns": 3,
        "image": "",
        "imagewidth": 120,
        "imageheight": 40,
        "tiles": [
          {
            "id": 0,
            "type": "floor-light",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#E8D5A3"
              }
            ]
          },
          {
            "id": 1,
            "type": "floor-dark",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#C9B88A"
              }
            ]
          },
          {
            "id": 2,
            "type": "wall",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#8B6914"
              }
            ]
          }
        ]
      }
    ]
  },
  "stage": {
    "id": "architecture",
    "stageNumber": 3,
    "title": "Building the Transformer",
    "roomWidth": 20,
    "roomHeight": 15,
    "concepts": [
      {
        "id": "encoder_stack",
        "title": "The Encoder: Understanding the Input",
        "content": "Key ideas:\n- 6 identical layers stacked\n- Each layer: self-attention → FFN\n- Both sublayers wrapped with residual + LayerNorm\n- Output: contextualized representations for all positions\n\nThe encoder's job: transform input tokens into rich, contextualized representations. It stacks 6 identical layers, each with two sublayers:\n\n1. **Multi-head self-attention**: Each position looks at all positions\n2. **Feed-forward network**: Process each position independently\n\nBoth sublayers are wrapped: output = LayerNorm(x + Sublayer(x))\n\nThink of the encoder as a series of increasingly sophisticated filters. Layer 1 captures local patterns. Layer 6 captures abstract, global relationships. The final encoder output goes to every decoder layer.\n\nWith d_model=512 maintained throughout, the encoder produces 512-dimensional vectors for each input position.",
        "position": {
          "x": 3,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "position_wise_ffn",
        "title": "Position-wise Feed-Forward Networks",
        "content": "Key ideas:\n- Applied to each position independently\n- Two linear transforms with ReLU: FFN(x) = max(0, xW₁+b₁)W₂+b₂\n- Inner dimension d_ff = 2048 (4× d_model)\n- Same across positions, different across layers\n\nAfter attention mixes information across positions, the FFN processes each position independently. It's deceptively simple:\n\n```\nFFN(x) = max(0, x·W₁ + b₁)·W₂ + b₂\n```\n\nThis is just two linear layers with a ReLU in between. The middle expands from 512 to 2048 dimensions, then projects back to 512. Think of it as each position getting to 'think' in a higher-dimensional space.\n\nWhy position-wise? The attention already mixed information. Now each position needs to process what it learned. The weights are shared across positions (like a 1×1 convolution) but different for each of the 6 layers.",
        "position": {
          "x": 6,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "masked_attention",
        "title": "Masked Self-Attention: No Peeking Ahead",
        "content": "Key ideas:\n- In decoder, prevent attending to future positions\n- Set future positions to -∞ before softmax\n- Preserves auto-regressive property\n- Position i can only attend to positions ≤ i\n\nDuring generation, the decoder produces tokens one at a time. It can't see the future—that would be cheating! Masked self-attention enforces this.\n\nBefore softmax, we add a mask that sets all 'illegal' attention scores to -∞:\n```\nattention_scores[i, j] = -∞  if j > i\n```\n\nAfter softmax, these become 0. Position 3 can only attend to positions 0, 1, 2, 3—never 4, 5, or beyond.\n\nThis triangular mask ensures the model learns to predict autoregressively: each position only depends on its left context. During training, we can still process all positions in parallel—we just mask the future.",
        "position": {
          "x": 9,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "decoder_stack",
        "title": "The Decoder: Generating the Output",
        "content": "Key ideas:\n- 6 identical layers stacked\n- Three sublayers per layer: masked self-attn → encoder-decoder attn → FFN\n- Masked attention preserves autoregressive property\n- Encoder-decoder attention reads from encoder output\n\nThe decoder generates output one token at a time, with three sublayers per layer:\n\n1. **Masked self-attention**: Attend to previous outputs only\n2. **Encoder-decoder attention**: Look at the input sequence\n3. **Feed-forward network**: Process independently\n\nEach sublayer has residual connections and layer norm. The decoder is more complex because it needs to:\n- Remember what it has generated (masked self-attention)\n- Know what the input said (encoder-decoder attention)\n- Process that combined information (FFN)\n\nThe encoder output is shared across all 6 decoder layers—computed once, used 6 times.",
        "position": {
          "x": 3,
          "y": 10
        },
        "type": "text"
      },
      {
        "id": "transformer_architecture",
        "title": "The Complete Transformer",
        "content": "Key ideas:\n- Encoder-decoder structure without recurrence\n- All attention-based: self-attention + cross-attention\n- Fully parallelizable during training\n- d_model=512, 6 layers, 8 heads in base model\n\nPutting it all together, the Transformer is:\n\n**Input side:**\n- Embed input tokens + add positional encoding\n- Pass through 6 encoder layers\n- Output: contextualized representations for all positions\n\n**Output side:**\n- Embed output tokens + add positional encoding\n- Pass through 6 decoder layers (with encoder-decoder attention)\n- Linear projection + softmax to get next token probabilities\n\nThe key insight: attention is all you need. No RNNs, no convolutions. Just attention layers stacked up. This enables full parallelization during training—all positions computed simultaneously.\n\nThe base model has 65M parameters. Training time on 8 GPUs: 12 hours. Result: new state-of-the-art on translation.",
        "position": {
          "x": 6,
          "y": 10
        },
        "type": "text"
      }
    ],
    "quiz": {
      "id": "quiz-stage-3",
      "question": "What did the Transformer eliminate compared to previous seq2seq models?",
      "type": "multiple-choice",
      "options": [
        "Attention",
        "Embeddings",
        "Recurrence"
      ],
      "correctAnswer": "Recurrence",
      "position": {
        "x": 18,
        "y": 7
      }
    },
    "doorPosition": {
      "x": 18,
      "y": 7
    },
    "spawnPosition": {
      "x": 1,
      "y": 7
    },
    "nextStage": 4,
    "previousStage": 2
  }
}