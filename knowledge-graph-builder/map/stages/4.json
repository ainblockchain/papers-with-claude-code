{
  "map": {
    "type": "map",
    "version": "1.10",
    "orientation": "orthogonal",
    "renderorder": "right-down",
    "width": 20,
    "height": 15,
    "tilewidth": 40,
    "tileheight": 40,
    "infinite": false,
    "layers": [
      {
        "type": "tilelayer",
        "id": 1,
        "name": "floor",
        "data": [
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3
        ],
        "width": 20,
        "height": 15,
        "x": 0,
        "y": 0,
        "visible": true,
        "opacity": 1
      },
      {
        "type": "tilelayer",
        "id": 2,
        "name": "collision",
        "data": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
        ],
        "width": 20,
        "height": 15,
        "x": 0,
        "y": 0,
        "visible": false,
        "opacity": 1
      },
      {
        "type": "objectgroup",
        "id": 3,
        "name": "objects",
        "draworder": "topdown",
        "x": 0,
        "y": 0,
        "visible": true,
        "opacity": 1,
        "objects": [
          {
            "id": 1,
            "name": "spawn",
            "type": "spawn",
            "x": 40,
            "y": 280,
            "width": 0,
            "height": 0,
            "rotation": 0,
            "visible": true,
            "point": true
          },
          {
            "id": 2,
            "name": "door",
            "type": "door",
            "x": 720,
            "y": 280,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true
          },
          {
            "id": 3,
            "name": "embedding_scaling",
            "type": "npc",
            "x": 120,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "embedding_scaling"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Embedding Scaling"
              }
            ]
          },
          {
            "id": 4,
            "name": "weight_tying",
            "type": "npc",
            "x": 240,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "weight_tying"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Weight Tying: Sharing Embeddings"
              }
            ]
          },
          {
            "id": 5,
            "name": "dropout_regularization",
            "type": "npc",
            "x": 360,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "dropout_regularization"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Dropout: Preventing Overfitting"
              }
            ]
          },
          {
            "id": 6,
            "name": "label_smoothing",
            "type": "npc",
            "x": 480,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "label_smoothing"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Label Smoothing: Embracing Uncertainty"
              }
            ]
          },
          {
            "id": 7,
            "name": "adam_optimizer",
            "type": "npc",
            "x": 120,
            "y": 400,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "adam_optimizer"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Adam with Warmup: A Custom Learning Rate"
              }
            ]
          },
          {
            "id": 8,
            "name": "beam_search",
            "type": "npc",
            "x": 240,
            "y": 400,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "beam_search"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Beam Search: Finding Good Translations"
              }
            ]
          },
          {
            "id": 9,
            "name": "model_parallelism",
            "type": "npc",
            "x": 360,
            "y": 400,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "model_parallelism"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Multi-GPU Training"
              }
            ]
          }
        ]
      }
    ],
    "tilesets": [
      {
        "firstgid": 1,
        "name": "course-room-tiles",
        "tilewidth": 40,
        "tileheight": 40,
        "tilecount": 3,
        "columns": 3,
        "image": "",
        "imagewidth": 120,
        "imageheight": 40,
        "tiles": [
          {
            "id": 0,
            "type": "floor-light",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#E8D5A3"
              }
            ]
          },
          {
            "id": 1,
            "type": "floor-dark",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#C9B88A"
              }
            ]
          },
          {
            "id": 2,
            "type": "wall",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#8B6914"
              }
            ]
          }
        ]
      }
    ]
  },
  "stage": {
    "id": "training_optimization",
    "stageNumber": 4,
    "title": "Training the Transformer",
    "roomWidth": 20,
    "roomHeight": 15,
    "concepts": [
      {
        "id": "embedding_scaling",
        "title": "Embedding Scaling",
        "content": "Key ideas:\n- Multiply embeddings by √d_model (√512 ≈ 22.6)\n- Balances embedding magnitude with positional encoding\n- Positional encodings have magnitude ~1\n- Scaled embeddings have similar magnitude\n\nA subtle but important detail: the paper multiplies token embeddings by √512 ≈ 22.6. Why?\n\nEmbeddings are typically initialized with small values (std ≈ 0.02). After scaling, they have magnitude ≈ 0.02 × 22.6 ≈ 0.45. Positional encodings are sines/cosines with magnitude ≈ 1.\n\nWithout scaling, positional information would dominate the token information. With scaling, they're roughly balanced, allowing both to contribute meaningfully.\n\nThink of it like mixing audio tracks: you want both the vocals (embeddings) and the beat (positions) to be audible, not one drowning out the other.",
        "position": {
          "x": 3,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "weight_tying",
        "title": "Weight Tying: Sharing Embeddings",
        "content": "Key ideas:\n- Same weights for input embeddings, output embeddings, and pre-softmax linear\n- Three-way weight sharing\n- Reduces parameters significantly\n- Embeddings learn to both encode and decode\n\nPress & Wolf (2017) showed that sharing the embedding weights improves language models. The Transformer takes this further with three-way sharing:\n\n1. **Input embedding**: token → 512-dim vector\n2. **Output embedding**: (same matrix)\n3. **Pre-softmax linear**: 512-dim → vocabulary logits (transpose of same matrix)\n\nThis makes intuitive sense: if 'cat' maps to vector v as input, then when the model wants to output 'cat', it should produce something similar to v.\n\nWith a 37K vocabulary and d=512, this saves ~38M parameters (37K × 512 × 2). The single embedding matrix learns a space where similar words are nearby—useful for both input and output.",
        "position": {
          "x": 6,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "dropout_regularization",
        "title": "Dropout: Preventing Overfitting",
        "content": "Key ideas:\n- P_drop = 0.1 in base model\n- Applied to: sublayer outputs, attention weights, embedding sums\n- Forces redundancy in learned representations\n- Essential for generalization\n\nDropout randomly zeros out neurons during training, preventing co-adaptation. The Transformer applies it liberally:\n\n- After each sublayer (before residual addition)\n- To attention weights (after softmax)\n- To the sum of embeddings + positional encoding\n\nWith P=0.1, 10% of values become zero on each forward pass. This forces the model to learn redundant representations—no single neuron can be critical.\n\nThink of it like training with random team members absent. The team learns to function without relying on any one person. During inference, no dropout—the full team works together.",
        "position": {
          "x": 9,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "label_smoothing",
        "title": "Label Smoothing: Embracing Uncertainty",
        "content": "Key ideas:\n- ε = 0.1 smoothing\n- Target: (1-ε) on correct class, ε/(V-1) on others\n- Hurts perplexity, improves BLEU\n- Prevents overconfident predictions\n\nStandard training pushes the model to output 1.0 probability for the correct answer and 0.0 for everything else. But this can make the model overconfident.\n\nLabel smoothing softens targets. Instead of [0, 0, 1, 0], use [0.033, 0.033, 0.9, 0.033] (with ε=0.1). The model learns 'pretty confident, but not absolutely certain.'\n\nInterestingly, this hurts perplexity (the model never predicts with 100% confidence) but improves BLEU score (translations are actually better). The lesson: maximizing the training metric isn't always the goal.",
        "position": {
          "x": 12,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "adam_optimizer",
        "title": "Adam with Warmup: A Custom Learning Rate",
        "content": "Key ideas:\n- Adam optimizer with β₁=0.9, β₂=0.98, ε=10⁻⁹\n- Linear warmup for first 4000 steps\n- Then decay proportional to step^(-0.5)\n- lr = d_model^(-0.5) × min(step^(-0.5), step × warmup_steps^(-1.5))\n\nThe Transformer's learning rate schedule is carefully designed:\n\n```\nlr = d_model^(-0.5) × min(step^(-0.5), step × warmup^(-1.5))\n```\n\n**Phase 1 (warmup)**: Learning rate increases linearly for 4000 steps. This prevents early instability—gradients are noisy initially, so take small steps.\n\n**Phase 2 (decay)**: Learning rate decreases with inverse square root of step number. As training progresses, make increasingly fine adjustments.\n\nWith d_model=512, the peak learning rate is about 0.0014 at step 4000. Think of it like heating metal: warm up slowly, then gradually cool for refined shaping.",
        "position": {
          "x": 3,
          "y": 10
        },
        "type": "text"
      },
      {
        "id": "beam_search",
        "title": "Beam Search: Finding Good Translations",
        "content": "Key ideas:\n- Keep k=4 best partial translations at each step\n- Length penalty α=0.6 to avoid short translations\n- Maximum length: input length + 50\n- Balance between exploration and efficiency\n\nDuring generation, greedy decoding picks the highest-probability token at each step. But this can lead to locally-optimal, globally-poor translations.\n\nBeam search maintains k=4 candidates. At each step, expand all candidates with all vocabulary tokens, score them, keep the top 4. It's like exploring 4 paths through a maze simultaneously.\n\nLength penalty prevents short translations (which have fewer opportunities to make mistakes, thus higher probability):\n```\nscore = log_prob / length^α\n```\n\nWith α=0.6, longer translations aren't unfairly penalized. The result: better translations than greedy, without exponential search cost.",
        "position": {
          "x": 6,
          "y": 10
        },
        "type": "text"
      },
      {
        "id": "model_parallelism",
        "title": "Multi-GPU Training",
        "content": "Key ideas:\n- Trained on 8 NVIDIA P100 GPUs\n- Base model: 100K steps in 12 hours\n- Big model: 300K steps in 3.5 days\n- Massive speedup vs recurrent models\n\nThe Transformer's parallelizability translates to dramatically faster training:\n\n**Base model (65M params):** 100K steps × ~0.4s/step = 12 hours on 8 P100 GPUs\n\n**Big model (213M params):** 300K steps = 3.5 days\n\nFor comparison, recurrent models trained for weeks or months. The secret: with no sequential dependencies, all positions compute simultaneously. 8 GPUs process 8 different batches, gradients are synchronized.\n\nThis speed advantage extends to inference too—though autoregressive generation is still sequential, each step is faster without recurrence overhead.",
        "position": {
          "x": 9,
          "y": 10
        },
        "type": "text"
      }
    ],
    "quiz": {
      "id": "quiz-stage-4",
      "question": "How long did it take to train the big Transformer model?",
      "type": "multiple-choice",
      "options": [
        "12 hours",
        "3.5 days",
        "2 weeks"
      ],
      "correctAnswer": "3.5 days",
      "position": {
        "x": 18,
        "y": 7
      }
    },
    "doorPosition": {
      "x": 18,
      "y": 7
    },
    "spawnPosition": {
      "x": 1,
      "y": 7
    },
    "nextStage": 5,
    "previousStage": 3
  }
}