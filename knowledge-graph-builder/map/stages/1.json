{
  "map": {
    "type": "map",
    "version": "1.10",
    "orientation": "orthogonal",
    "renderorder": "right-down",
    "width": 20,
    "height": 15,
    "tilewidth": 40,
    "tileheight": 40,
    "infinite": false,
    "layers": [
      {
        "type": "tilelayer",
        "id": 1,
        "name": "floor",
        "data": [
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3
        ],
        "width": 20,
        "height": 15,
        "x": 0,
        "y": 0,
        "visible": true,
        "opacity": 1
      },
      {
        "type": "tilelayer",
        "id": 2,
        "name": "collision",
        "data": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
        ],
        "width": 20,
        "height": 15,
        "x": 0,
        "y": 0,
        "visible": false,
        "opacity": 1
      },
      {
        "type": "objectgroup",
        "id": 3,
        "name": "objects",
        "draworder": "topdown",
        "x": 0,
        "y": 0,
        "visible": true,
        "opacity": 1,
        "objects": [
          {
            "id": 1,
            "name": "spawn",
            "type": "spawn",
            "x": 40,
            "y": 280,
            "width": 0,
            "height": 0,
            "rotation": 0,
            "visible": true,
            "point": true
          },
          {
            "id": 2,
            "name": "door",
            "type": "door",
            "x": 720,
            "y": 280,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true
          },
          {
            "id": 3,
            "name": "sequence_to_sequence",
            "type": "npc",
            "x": 120,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "sequence_to_sequence"
              },
              {
                "name": "title",
                "type": "string",
                "value": "The Seq2Seq Revolution"
              }
            ]
          },
          {
            "id": 4,
            "name": "attention_mechanism",
            "type": "npc",
            "x": 240,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "attention_mechanism"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Attention: Looking Back at the Input"
              }
            ]
          },
          {
            "id": 5,
            "name": "recurrent_limitations",
            "type": "npc",
            "x": 360,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "recurrent_limitations"
              },
              {
                "name": "title",
                "type": "string",
                "value": "The Bottleneck of Sequential Processing"
              }
            ]
          },
          {
            "id": 6,
            "name": "residual_connections",
            "type": "npc",
            "x": 120,
            "y": 400,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "residual_connections"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Skip Connections: Gradient Highways"
              }
            ]
          },
          {
            "id": 7,
            "name": "layer_normalization",
            "type": "npc",
            "x": 240,
            "y": 400,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "layer_normalization"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Layer Normalization: Stabilizing Training"
              }
            ]
          },
          {
            "id": 8,
            "name": "byte_pair_encoding",
            "type": "npc",
            "x": 360,
            "y": 400,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "byte_pair_encoding"
              },
              {
                "name": "title",
                "type": "string",
                "value": "BPE: Subword Tokenization"
              }
            ]
          }
        ]
      }
    ],
    "tilesets": [
      {
        "firstgid": 1,
        "name": "course-room-tiles",
        "tilewidth": 40,
        "tileheight": 40,
        "tilecount": 3,
        "columns": 3,
        "image": "",
        "imagewidth": 120,
        "imageheight": 40,
        "tiles": [
          {
            "id": 0,
            "type": "floor-light",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#E8D5A3"
              }
            ]
          },
          {
            "id": 1,
            "type": "floor-dark",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#C9B88A"
              }
            ]
          },
          {
            "id": 2,
            "type": "wall",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#8B6914"
              }
            ]
          }
        ]
      }
    ]
  },
  "stage": {
    "id": "foundations",
    "stageNumber": 1,
    "title": "Foundations: Why We Needed Transformers",
    "roomWidth": 20,
    "roomHeight": 15,
    "concepts": [
      {
        "id": "sequence_to_sequence",
        "title": "The Seq2Seq Revolution",
        "content": "Key ideas:\n- Encoder compresses input sequence into a context vector\n- Decoder generates output sequence from context\n- Originally used RNNs/LSTMs for both components\n- Enabled breakthroughs in machine translation\n\nIn 2014, Sutskever and colleagues at Google introduced a paradigm-shifting idea: use one neural network to read an entire sentence, compress it into a vector, then use another network to generate the translation.\n\nThink of it like a game of telephone, but with a really good memory. The encoder 'listens' to the entire input sentence and summarizes it into a dense vector. The decoder then 'speaks' the translation based solely on that summary.\n\nThis encoder-decoder pattern became the foundation for all modern translation systems. But there was a catch: squeezing a whole sentence into a single vector created an information bottleneck, especially for long sentences.",
        "position": {
          "x": 3,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "attention_mechanism",
        "title": "Attention: Looking Back at the Input",
        "content": "Key ideas:\n- Allows decoder to 'look back' at all encoder states\n- Computes relevance scores between current output and all inputs\n- Produces weighted sum of encoder representations\n- Solved the information bottleneck problem\n\nIn 2015, Bahdanau realized the fatal flaw in seq2seq: cramming a 50-word sentence into a single vector loses information. His solution was elegant—let the decoder 'peek' back at the encoder.\n\nImagine translating a long document. Instead of memorizing the whole thing before translating, you'd keep the original open and glance back when needed. That's attention: for each output word, compute how relevant each input word is, then focus on the important ones.\n\nThe decoder asks 'which input words should I pay attention to right now?' and gets a weighted combination of all encoder states. This single idea—letting the model decide where to look—revolutionized NLP.",
        "position": {
          "x": 6,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "recurrent_limitations",
        "title": "The Bottleneck of Sequential Processing",
        "content": "Key ideas:\n- RNNs process one token at a time, sequentially\n- Cannot parallelize: must wait for previous step\n- Long-range dependencies suffer from vanishing gradients\n- Training is slow and scaling is difficult\n\nEven with attention, seq2seq models had a fundamental problem: they were built on RNNs, which process sequences one step at a time. To encode 'The cat sat on the mat', you must process 'The', then 'cat', then 'sat'... in order.\n\nThis is like being forced to read a book one word at a time, never looking ahead. It's slow (no parallelization), and by the time you reach the end, you've partially forgotten the beginning (vanishing gradients).\n\nVaswani et al. asked: what if we didn't need recurrence at all? What if attention alone could capture sequential relationships? This question led to the Transformer.",
        "position": {
          "x": 9,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "residual_connections",
        "title": "Skip Connections: Gradient Highways",
        "content": "Key ideas:\n- Add input directly to sublayer output: x + Sublayer(x)\n- Creates 'shortcut' paths for gradients during backprop\n- Enables training of very deep networks\n- Essential for stacking many Transformer layers\n\nIn 2016, He et al. discovered why very deep networks were hard to train: gradients vanish as they backpropagate through many layers. Their solution was brilliantly simple—add skip connections.\n\nInstead of computing y = F(x), compute y = x + F(x). Now the gradient can flow directly through the addition, bypassing the layer entirely if needed. It's like building a highway with express lanes that skip traffic jams.\n\nThe Transformer uses these everywhere. With 6+ stacked layers, residual connections ensure gradients reach the earliest layers without fading away.",
        "position": {
          "x": 3,
          "y": 10
        },
        "type": "text"
      },
      {
        "id": "layer_normalization",
        "title": "Layer Normalization: Stabilizing Training",
        "content": "Key ideas:\n- Normalizes activations across the feature dimension\n- Applied after residual addition in each sublayer\n- Stabilizes training and enables higher learning rates\n- Independent of batch size (unlike batch normalization)\n\nBa et al. (2016) introduced layer normalization to stabilize RNN training. Unlike batch normalization (which normalizes across samples), layer norm normalizes across features for each sample independently.\n\nThink of it like adjusting the volume on each speaker in a concert separately, regardless of how loud the other speakers are. Each token's representation gets normalized to have zero mean and unit variance across its 512 dimensions.\n\nIn Transformers, LayerNorm is applied after each residual connection: LayerNorm(x + Sublayer(x)). This keeps activations in a stable range, preventing exploding or vanishing values.",
        "position": {
          "x": 6,
          "y": 10
        },
        "type": "text"
      },
      {
        "id": "byte_pair_encoding",
        "title": "BPE: Subword Tokenization",
        "content": "Key ideas:\n- Iteratively merges frequent character pairs into tokens\n- Creates vocabulary between character-level and word-level\n- Handles rare/unknown words through subword decomposition\n- 37K shared vocabulary for source and target languages\n\nSennrich et al. (2016) solved the out-of-vocabulary problem with Byte Pair Encoding. The idea comes from data compression: repeatedly merge the most frequent adjacent pair of tokens.\n\nStart with characters. 'th' appears often? Merge to single token. 'the' appears often? Merge again. Eventually you get a vocabulary that represents common words as single tokens but can decompose rare words into pieces.\n\nThe Transformer paper uses BPE with 37,000 tokens shared between English and German. 'unhappiness' might become ['un', 'happiness'], while 'the' stays as one token. This elegantly handles any word the model encounters.",
        "position": {
          "x": 9,
          "y": 10
        },
        "type": "text"
      }
    ],
    "quiz": {
      "id": "quiz-stage-1",
      "question": "How would BPE likely tokenize a rare word like 'unparallelizable'?",
      "type": "multiple-choice",
      "options": [
        "As a single token",
        "As individual characters",
        "As subword pieces like 'un' + 'parallel' + 'izable'"
      ],
      "correctAnswer": "As subword pieces like 'un' + 'parallel' + 'izable'",
      "position": {
        "x": 18,
        "y": 7
      }
    },
    "doorPosition": {
      "x": 18,
      "y": 7
    },
    "spawnPosition": {
      "x": 1,
      "y": 7
    },
    "nextStage": 2,
    "previousStage": null
  }
}