{
  "map": {
    "type": "map",
    "version": "1.10",
    "orientation": "orthogonal",
    "renderorder": "right-down",
    "width": 20,
    "height": 15,
    "tilewidth": 40,
    "tileheight": 40,
    "infinite": false,
    "layers": [
      {
        "type": "tilelayer",
        "id": 1,
        "name": "floor",
        "data": [
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          3,
          3,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          1,
          2,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3,
          3
        ],
        "width": 20,
        "height": 15,
        "x": 0,
        "y": 0,
        "visible": true,
        "opacity": 1
      },
      {
        "type": "tilelayer",
        "id": 2,
        "name": "collision",
        "data": [
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          0,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1,
          1
        ],
        "width": 20,
        "height": 15,
        "x": 0,
        "y": 0,
        "visible": false,
        "opacity": 1
      },
      {
        "type": "objectgroup",
        "id": 3,
        "name": "objects",
        "draworder": "topdown",
        "x": 0,
        "y": 0,
        "visible": true,
        "opacity": 1,
        "objects": [
          {
            "id": 1,
            "name": "spawn",
            "type": "spawn",
            "x": 40,
            "y": 280,
            "width": 0,
            "height": 0,
            "rotation": 0,
            "visible": true,
            "point": true
          },
          {
            "id": 2,
            "name": "door",
            "type": "door",
            "x": 720,
            "y": 280,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true
          },
          {
            "id": 3,
            "name": "scaled_dot_product_attention",
            "type": "npc",
            "x": 120,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "scaled_dot_product_attention"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Scaled Dot-Product Attention"
              }
            ]
          },
          {
            "id": 4,
            "name": "multi_head_attention",
            "type": "npc",
            "x": 240,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "multi_head_attention"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Multi-Head Attention: Parallel Perspectives"
              }
            ]
          },
          {
            "id": 5,
            "name": "self_attention",
            "type": "npc",
            "x": 360,
            "y": 160,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "self_attention"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Self-Attention: Relating Positions Within a Sequence"
              }
            ]
          },
          {
            "id": 6,
            "name": "encoder_decoder_attention",
            "type": "npc",
            "x": 120,
            "y": 400,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "encoder_decoder_attention"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Encoder-Decoder Attention: Bridging the Gap"
              }
            ]
          },
          {
            "id": 7,
            "name": "positional_encoding",
            "type": "npc",
            "x": 240,
            "y": 400,
            "width": 40,
            "height": 40,
            "rotation": 0,
            "visible": true,
            "properties": [
              {
                "name": "conceptId",
                "type": "string",
                "value": "positional_encoding"
              },
              {
                "name": "title",
                "type": "string",
                "value": "Positional Encoding: Teaching Order"
              }
            ]
          }
        ]
      }
    ],
    "tilesets": [
      {
        "firstgid": 1,
        "name": "course-room-tiles",
        "tilewidth": 40,
        "tileheight": 40,
        "tilecount": 3,
        "columns": 3,
        "image": "",
        "imagewidth": 120,
        "imageheight": 40,
        "tiles": [
          {
            "id": 0,
            "type": "floor-light",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#E8D5A3"
              }
            ]
          },
          {
            "id": 1,
            "type": "floor-dark",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#C9B88A"
              }
            ]
          },
          {
            "id": 2,
            "type": "wall",
            "properties": [
              {
                "name": "color",
                "type": "string",
                "value": "#8B6914"
              }
            ]
          }
        ]
      }
    ]
  },
  "stage": {
    "id": "core_attention",
    "stageNumber": 2,
    "title": "The Attention Revolution",
    "roomWidth": 20,
    "roomHeight": 15,
    "concepts": [
      {
        "id": "scaled_dot_product_attention",
        "title": "Scaled Dot-Product Attention",
        "content": "Key ideas:\n- Attention(Q,K,V) = softmax(QK^T / √d_k) × V\n- Queries ask 'what am I looking for?'\n- Keys answer 'what do I contain?'\n- Values are 'what do I actually output?'\n- Scaling by √d_k prevents softmax saturation\n\nVaswani et al. formalized attention with three components: Queries (Q), Keys (K), and Values (V). Think of a library: queries are your search terms, keys are book titles, and values are the book contents.\n\nThe formula is elegant:\n```\nAttention(Q,K,V) = softmax(QK^T / √d_k) × V\n```\n\n1. Compute similarity: QK^T gives a score for each query-key pair\n2. Scale: divide by √d_k (here, √64 = 8) to keep values manageable\n3. Normalize: softmax turns scores into probabilities\n4. Aggregate: weighted sum of values\n\nWhy scale? If d_k is large, dot products can be huge, pushing softmax into regions where gradients vanish. Scaling keeps things in a happy range.",
        "position": {
          "x": 3,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "multi_head_attention",
        "title": "Multi-Head Attention: Parallel Perspectives",
        "content": "Key ideas:\n- Run h=8 attention operations in parallel\n- Each head uses different learned projections\n- Concatenate all head outputs, then project\n- Allows attending to different representation subspaces\n\nOne attention head can only focus on one pattern at a time. But language has many simultaneous relationships: syntax, semantics, coreference, etc. Solution: use multiple heads in parallel.\n\nEach head has its own learned projection matrices. With d_model=512 and 8 heads, each head works with d_k=64 dimensions. It's like having 8 different experts, each examining the sequence from their own angle.\n\n```\nMultiHead(Q,K,V) = Concat(head_1,...,head_8) × W^O\n```\n\nThe outputs are concatenated back to 512 dimensions and mixed with a final projection. Different heads learn different things: one might track subject-verb agreement, another might handle coreference.",
        "position": {
          "x": 6,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "self_attention",
        "title": "Self-Attention: Relating Positions Within a Sequence",
        "content": "Key ideas:\n- Q, K, V all derived from the same sequence\n- Every position can attend to every other position\n- Captures dependencies regardless of distance\n- Path length between any two positions: O(1)\n\nSelf-attention is where the magic happens. Unlike cross-attention (where one sequence attends to another), self-attention lets a sequence attend to itself.\n\nGiven input X, we compute Q = XW^Q, K = XW^K, V = XW^V. Each position asks: 'What other positions in my own sequence should I pay attention to?'\n\nThis is revolutionary because any position can directly attend to any other—no passing through intermediate steps. In 'The cat sat on the mat because it was tired', 'it' can directly attend to 'cat' in one step, not through a chain of hidden states.\n\nThis O(1) path length is why Transformers capture long-range dependencies so well.",
        "position": {
          "x": 9,
          "y": 4
        },
        "type": "text"
      },
      {
        "id": "encoder_decoder_attention",
        "title": "Encoder-Decoder Attention: Bridging the Gap",
        "content": "Key ideas:\n- Queries come from decoder, keys/values from encoder\n- Every decoder position attends to all encoder positions\n- This is where translation alignment happens\n- Mimics the attention mechanism in seq2seq\n\nSelf-attention lets each side understand itself, but translation requires the decoder to 'see' the encoder. That's encoder-decoder attention.\n\nIn each decoder layer, after masked self-attention, there's a cross-attention sublayer:\n- Queries: from decoder's current state\n- Keys & Values: from encoder's final output\n\nThis is where 'le chat' learns to look at 'the cat'. The decoder asks: 'Given what I've generated so far, which parts of the input should I focus on now?'\n\nIt's the same multi-head attention mechanism, just with Q from one side and K,V from the other.",
        "position": {
          "x": 3,
          "y": 10
        },
        "type": "text"
      },
      {
        "id": "positional_encoding",
        "title": "Positional Encoding: Teaching Order",
        "content": "Key ideas:\n- Attention is permutation-invariant: needs position info\n- Add sinusoidal signals to embeddings\n- PE(pos,2i) = sin(pos / 10000^(2i/d))\n- PE(pos,2i+1) = cos(pos / 10000^(2i/d))\n- Relative positions encoded as linear functions\n\nHere's a problem: attention doesn't know about order. 'Dog bites man' and 'Man bites dog' would produce identical attention patterns! We need to inject position information.\n\nVaswani et al. chose sinusoidal functions at different frequencies:\n```\nPE(pos, 2i)   = sin(pos / 10000^(2i/512))\nPE(pos, 2i+1) = cos(pos / 10000^(2i/512))\n```\n\nThink of it like a clock with many hands rotating at different speeds. Position 0 has a unique pattern of sine/cosine values; position 1 has a slightly different pattern. The beauty: PE(pos+k) can be expressed as a linear function of PE(pos), so the model can learn relative positions.\n\nThese encodings are added (not concatenated) to the input embeddings.",
        "position": {
          "x": 6,
          "y": 10
        },
        "type": "text"
      }
    ],
    "quiz": {
      "id": "quiz-stage-2",
      "question": "Why do Transformers need positional encoding?",
      "type": "multiple-choice",
      "options": [
        "To make the model bigger",
        "Because attention treats sequences as unordered sets",
        "To improve memory efficiency"
      ],
      "correctAnswer": "Because attention treats sequences as unordered sets",
      "position": {
        "x": 18,
        "y": 7
      }
    },
    "doorPosition": {
      "x": 18,
      "y": 7
    },
    "spawnPosition": {
      "x": 1,
      "y": 7
    },
    "nextStage": 3,
    "previousStage": 1
  }
}